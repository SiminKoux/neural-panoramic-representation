# neural-panoramic-representation


Content-based 360째 video editing allows users to manipulate panoramic content in a dynamic visual world. However, existing learning-based video representation methods show limitations in producing high-quality panoramic dynamic content from 360째 video due to their lack of capacity to model the inherent spatiotemporal relationships among pixels in the true panoramic space. To address this issue, we propose Neural Panoramic Representation (NPR), a novel method utilizing MLPs to learn implicit spherical content layers by encoding both the spherical spatiotemporal positions and appearance details within 360째 videos. Our designed bi-directional mapping between the original video frames and the learned content layers enables the capture of interpretable and global omnidirectional visual characteristics of dynamic scenes. Additionally, we introduce two innovative constraints for spherical preservation to ensure the creation of appropriate implicit spherical content layers. We further provide an interactive layer neural panoramic editing approach based on the proposed NPR. Experiments on diverse real-world 360째 videos show superior performance compared to existing state-of-the-art neural representation techniques.
